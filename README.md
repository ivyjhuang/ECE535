# ECE535
How our team is suitable:  
Alice took CS 383, an artificial intelligence class, so she has some background in ML. We both learned a little about ML in ECE 371, Security Engineering. We also both attended an ML workshop at M5. We are interested in gaining a better understanding of ML.

Motivation:  
To understand how federated learning works on multimodal sensory data. We are interested in federated learning, where the training is distributed across multiple devices instead of the traditional ML where training takes place in only one device. This project will help us get more comfortable with ML. 

Design Goals:
Understand the Federated Learning code and add our explanation for it on our GitHub repository and   
and comments to the code so anyone looking at the Federated Learning code for the first time can understand it  

Deliverables:  
- Understand multimodal federated learning (code:https://github.com/yuchenzhao/iotdi22-mmfl)   
- Use given datasets to reproduce the results in the paper
- data: https://drive.google.com/drive/folders/1rWJYkfMavGs1F-H0jykJ5V0fIiwrQdJV   
- Perform a per-class accuracy analysis of the results and observe the effect of skewed data distribution on the per-class accuracy   
- Evaluate the system on a multimodal dataset that is relatively balanced in class distribution  

System blocks:   ![IMG_0993](https://github.com/ivyjhuang/ECE535/assets/89998421/e08b622b-f7ee-469f-bbf4-30482d89b1a1)



HW/SW requirements:  
There are no hardware requirements, only software requirements. The code and data set is provided in the Github repository. We will be testing and getting results with the Python code provided. 

Team members responsibilities:  
- Setup: Alice and Ivy
- Software: Alice and Ivy
- Networking: Alice
- Writing: Ivy
- Research: Alice
- Algorithm Design: Alice and Ivy   

Project timeline:  
- Familiarize ourselves with the code and get the setup done by Oct. 5  
- Work about 1- 5 hrs/week  
- Finish a week before the due date   

Results: The graphs from our project, both the reproduced graphs from the paper, the per class accuracy graphs, and the data distribution graphs are included below. Descriptions of them are in the paper. 

Reproduced graphs:

<img width="464" alt="Screen Shot 2023-12-15 at 12 22 18 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/bc47755d-bffd-4f7d-9701-3885bf2ca97c">
<img width="595" alt="Screen Shot 2023-12-15 at 12 14 47 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/db81f9f5-a13f-44fe-9fce-ae9d479e5ea6">
<img width="593" alt="Screen Shot 2023-12-15 at 12 12 37 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/a1b3f0cc-2af7-4486-8c9b-03e8b2d5184a">

[single_multi_modality_comparison_mHealth_split_ae_gyro_mage.pdf](https://github.com/ivyjhuang/ECE535/files/13691237/single_multi_modality_comparison_mHealth_split_ae_gyro_mage.pdf)

<img width="538" alt="Screen Shot 2023-12-15 at 12 26 16 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/04536499-20dd-4284-b3dc-17b7227ac0e5">

[single_multi_modality_comparison_UR_Fall_split_ae_rgb_depth.pdf](https://github.com/ivyjhuang/ECE535/files/13691280/single_multi_modality_comparison_UR_Fall_split_ae_rgb_depth.pdf)


<img width="708" alt="Screen Shot 2023-12-15 at 2 21 40 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/25efa890-fcf6-433f-a424-3aaef6d29cfb">

<img width="681" alt="Screen Shot 2023-12-15 at 2 27 38 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/43e746b2-8100-4eab-9f35-b6288d24aa26">

<img width="755" alt="Screen Shot 2023-12-15 at 2 32 25 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/b44f1a87-94e0-4aa8-9f02-d3a0a945365f">

<img width="616" alt="Screen Shot 2023-12-15 at 2 35 52 PM" src="https://github.com/ivyjhuang/ECE535/assets/146273691/09f43c98-fdac-4604-9a44-374fd983a4df">











References:  
Multimodal Federated Learning on IOT Data (https://pure-research.york.ac.uk/ws/portalfiles/portal/ 79047763/2109.04833v2.pdf)  
Communication-Efficient learning of deep networks from decentralized data (http:// proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf)  



